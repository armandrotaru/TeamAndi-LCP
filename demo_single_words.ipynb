{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for single words as targets\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<span style=\"color:red\">Before you run the demo, make sure to follow the steps from the README.md file.</span>\n",
    "\n",
    "<span style=\"color:red\">If you want to learn more about the underlying implementation, use the help command.</span>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary libraries, and (optionally) set the cache folder for the context-dependent models (i.e., Hugging Face transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['TRANSFORMERS_CACHE'] = <new_cache_folder_path>\n",
    "\n",
    "import csv\n",
    "import time\n",
    "  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sources.final_model import LcpRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the ridge regression model (i.e., LcpRegressor), and specify that the targets consist of single words (i.e., use_single_words=True). Enforce a strong degree of regularization (i.e., lambda_param=1200), and run the model in verbose mode (i.e., verbose=True), since this allows the detection of potential bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model = LcpRegressor(use_single_words=True, lambda_param=1200, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process all the behavioural norms and distributional models. Alternatively, you can process only a subset of norms, for instance by excluding those that have low predictive power and/or take too much time to load. You must assign a name to each norm/model, and it is this name that you will later use if you wish to generate predictors based on that particular norm/model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the behavioural norms from file. If you do not plan to use norms at all, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_norm_names = ['Conc', 'SemD', 'Freq_CD', 'Prev', 'AoA', 'Emo', 'SensMot', 'Comp', 'MRC', 'LD']\n",
    "behav_norm_filenames = ['Concreteness norms.txt', \n",
    "                        'Semantic diversity norms.txt',\n",
    "                        'Frequency and contextual diversity norms.txt',\n",
    "                        'Prevalence norms.txt',\n",
    "                        'Age of acquisition norms.txt',\n",
    "                        'Emotional norms.txt',\n",
    "                        'Sensorimotor norms.txt',\n",
    "                        'Complexity norms.txt',\n",
    "                        'MRC norms.txt', \n",
    "                        'Lexical decision norms.txt']\n",
    "\n",
    "curr_model.load_behav_norms(behav_norm_names, behav_norm_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the context-independent models from file. If you do not plan to use context-independent models at all, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_indep_model_names = ['Skip-gram', 'GloVe', 'NumberBatch']\n",
    "cont_indep_model_filenames = ['Skip-gram embeddings.txt',\n",
    "                              'GloVe embeddings.txt',\n",
    "                              'ConceptNet NumberBatch embeddings.txt']\n",
    "\n",
    "curr_model.load_cont_indep_models(cont_indep_model_names, cont_indep_model_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the (pre-trained) context-dependent models, using the Hugging Face library. The classes of models (i.e., transformers) currently supported by our implementation are 'albert', 'bert', 'deberta', 'electra', and 'roberta'. Each class has one or more available models (e.g., in the case of BERT, valid ids are 'bert-base-uncased', 'bert-base-cased', 'bert-large-cased', etc.; you can find the full list at https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_dep_model_names = ['Albert', 'Bert', 'Deberta', 'Electra_small', 'Electra_base', 'Electra_large', 'Roberta']\n",
    "cont_dep_model_ids = ['albert-base-v2', \n",
    "                      'bert-base-uncased', \n",
    "                      'microsoft/deberta-base', \n",
    "                      'google/electra-small-discriminator',\n",
    "                      'google/electra-base-discriminator',\n",
    "                      'google/electra-large-discriminator',\n",
    "                      'roberta-base']\n",
    "\n",
    "curr_model.load_cont_dep_models(cont_dep_model_names, cont_dep_model_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previously loaded norms/models and their corresponding names, select one or more types of predictors that will be used in fitting the complexity ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_names = ['Conc', 'SemD', 'Freq_CD', 'Prev', 'AoA', 'Emo', 'SensMot', 'Comp', 'MRC', 'LD',\n",
    "              'Skip-gram', 'GloVe', 'NumberBatch',\n",
    "              'Albert', 'Bert', 'Deberta', 'Electra_small', 'Electra_base', 'Electra_large', 'Roberta']\n",
    "\n",
    "curr_model.select_preds(pred_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the train and test datasets from file. Like in the case of the norms and models, you are free to provide your own set of stimuli, as long as they follow the format employed by the organizers of LCP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_train = pd.read_csv('./stimuli/lcp_single_train.tsv', delimiter='\\t', quoting=csv.QUOTE_NONE, na_filter=False) \n",
    "y_train = stimuli_train['complexity'];\n",
    "X_train = stimuli_train.drop(['complexity'], axis=1)\n",
    "\n",
    "stimuli_test = pd.read_csv('./stimuli/lcp_single_test.tsv', delimiter='\\t', quoting=csv.QUOTE_NONE, na_filter=False) \n",
    "y_test = stimuli_test['complexity'];\n",
    "X_test = stimuli_test.drop(['complexity'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to the train dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_list = curr_model.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, evaluate model performance over the test dataset, using Pearson and Spearman correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr, spearman_corr = curr_model.score(X_test, y_test)\n",
    "\n",
    "print('Pearson correlation (test set): {:.2f}'.format(pearson_corr))\n",
    "print('Spearman correlation (test set): {:.2f}'.format(spearman_corr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
